{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e736add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # avoid tensorflow warning\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "if \"notebooks\" in os.getcwd():\n",
    "    main_dir = os.getcwd()[:-10]\n",
    "    os.chdir(\"../scripts/GPA_NN\")\n",
    "elif \"/scripts/GPA_NN\" in os.getcwd():\n",
    "    main_dir = os.getcwd()[:-15]\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "# input parameters --------------------------------------\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='Learning_gaussian')\n",
    "parser.add_argument('--generative_model', type=str, default=\"GPA_NN\")\n",
    "p, _ = parser.parse_known_args()\n",
    "\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "yaml_file = \"../../configs/{dataset}-{generative_model}.yaml\".format(\n",
    "    dataset=p.dataset, generative_model=p.generative_model)\n",
    "\n",
    "\n",
    "with open(yaml_file, 'r') as f:\n",
    "    param = yaml.load(f, Loader=SafeLoader)\n",
    "    #print(param)\n",
    "    \n",
    "updated_param = vars(p)\n",
    "for param_key, param_val in updated_param.items():\n",
    "    if type(param_val) == type(None):\n",
    "        continue\n",
    "    param[param_key] = param_val\n",
    "    \n",
    "if param['alpha']:\n",
    "    par = [param['alpha']]\n",
    "    param['exptype'] = '%s=%05.2f-%s' % (param['f'], param['alpha'], param['Gamma'])\n",
    "else: \n",
    "    par = []\n",
    "    param['exptype'] = '%s-%s' % (param['f'], param['Gamma'])\n",
    "if param['L'] == None:\n",
    "    param['expname'] = '%s_%s' % (param['exptype'], 'inf')\n",
    "else:\n",
    "    param['expname'] = '%s_%.4f' % (param['exptype'], param['L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5559e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "Data prepared.\n"
     ]
    }
   ],
   "source": [
    "# Data generation ----------------------------------------\n",
    "from scripts.util.generate_data import generate_data\n",
    "param, X_, Y_, X_label, Y_label = generate_data(param)\n",
    "\n",
    "Q = tf.constant(X_, dtype=tf.float32) # constant\n",
    "P = tf.Variable(Y_, dtype=tf.float32) # variable\n",
    "    \n",
    "Q_label, P_label = None, None\n",
    "data_par = {'P_label': P_label, 'Q_label': Q_label, 'mb_size_P': param['mb_size_P'],\n",
    "            'mb_size_Q': param['mb_size_Q'], 'N_samples_P': param['N_samples_P'], \n",
    "            'N_samples_Q': param['N_samples_Q'], }\n",
    "\n",
    "print(\"Data prepared.\")\n",
    "\n",
    "#from scripts.util.plot_result import plot_initial_data\n",
    "#plot_initial_data(X_ = X_, Y_ = Y_, proj_axes = [0,1], x_lim = [None, None], y_lim = [None, None], show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3d40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator learning  -----------------------------------------\n",
    "# Discriminator construction using Neural Network\n",
    "from scripts.GPA_NN.lib.construct_NN import check_nn_topology, initialize_NN, model\n",
    "\n",
    "N_fnn_layers, N_cnn_layers, param['activation_ftn'] = check_nn_topology(\n",
    "    param['NN_model'], param['N_fnn_layers'], param['N_cnn_layers'], param['N_dim'], param['activation_ftn'])\n",
    "\n",
    "NN_par = {'NN_model':param['NN_model'], 'activation_ftn':param['activation_ftn'], \n",
    "          'N_dim': param['N_dim'], 'N_cnn_layers':N_cnn_layers, 'N_fnn_layers':N_fnn_layers, \n",
    "          'N_conditions': param['N_conditions'], 'constraint': param['constraint'], \n",
    "          'L': param['L'], 'eps': param['eps']}\n",
    "\n",
    "W, b = initialize_NN(NN_par)\n",
    "phi = model(NN_par)  # discriminator\n",
    "\n",
    "# scalar optimal value optimization for f-divergence\n",
    "nu = tf.Variable(0.0, dtype=tf.float32)\n",
    "\n",
    "parameters = {'W':W, 'b':b, 'nu':nu} # Learnable parameters for the discriminator phi\n",
    "\n",
    "# Train setting\n",
    "from scripts.GPA_NN.lib.train_NN import train_disc\n",
    "lr_phi = tf.Variable(param['lr_phi'], trainable=False) # lr for training a discriminator function\n",
    "\n",
    "# (Discriminator) Loss ----------------------------------------------\n",
    "loss_par = {'f': param['f'], 'formulation': param['formulation'], 'par': par, \n",
    "            'reverse': param['reverse'], 'lamda': param['lamda']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0726cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transporting particles --------------------------------------------\n",
    "# ODE solver setting\n",
    "from lib.transport_particles import calc_vectorfield, solve_ode\n",
    "dPs = []\n",
    "if param['ode_solver'] in ['forward_euler', 'AB2', 'AB3', 'AB4', 'AB5']:\n",
    "    aux_params = []\n",
    "else:\n",
    "    aux_params = {'parameters': parameters, 'phi': phi, 'Q': Q, 'lr_phi': lr_phi,'epochs_phi': param['epochs_phi'], 'loss_par': loss_par, 'NN_par': NN_par, 'data_par': data_par, 'optimizer': param['optimizer']}\n",
    "\n",
    "# Train setting\n",
    "lr_P_init = param['lr_P'] # Assume that deltat = deltat(t)\n",
    "lr_P = tf.Variable(lr_P_init, trainable=False)\n",
    "lr_Ps = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d806b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Wasserstein-1 metric ----------------------------------\n",
    "if param['calc_Wasserstein1'] == True:\n",
    "    NN_par2 = {'NN_model':param['NN_model'], 'activation_ftn':param['activation_ftn'], \n",
    "               'N_dim': param['N_dim'], 'N_cnn_layers':N_cnn_layers, 'N_fnn_layers':N_fnn_layers, \n",
    "               'N_conditions': param['N_conditions'], 'constraint': param['constraint'], \n",
    "               'L': 1.0, 'eps': param['eps']}\n",
    "\n",
    "    W2, b2 = initialize_NN(NN_par2)\n",
    "    phi2 = model(NN_par2)  # discriminator for Wasserstein 1 metric\n",
    "    parameters2 = {'W':W2, 'b':b2} # Learnable parameters for the discriminator phi2\n",
    "    \n",
    "    # Train setting\n",
    "    from lib.train_NN import train_wasserstein1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b37bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & plot settings -----------------------------------------------\n",
    "# Metrics to calculate\n",
    "from scripts.util.evaluate_metric import calc_ke, calc_grad_phi\n",
    "if np.prod(param['N_dim']) <= 12:\n",
    "    from scripts.util.evaluate_metric import calc_sinkhorn\n",
    "trajectories = []\n",
    "vectorfields = []\n",
    "divergences = []\n",
    "wasserstein1s = []\n",
    "KE_Ps = []\n",
    "FIDs = []\n",
    "\n",
    "# saving/plotting parameters\n",
    "if param['save_iter'] >= param['epochs']:\n",
    "    param['save_iter'] = 1\n",
    "\n",
    "if param['plot_result'] == True:\n",
    "    from scripts.util.plot_result import plot_result\n",
    "\n",
    "if not os.path.exists(main_dir + '/assets/' + param['dataset']):\n",
    "    os.makedirs(main_dir + '/assets/' + param['dataset'])\n",
    "\n",
    "param['expname'] = param['expname']+'_%04d_%04d_%02d_%s' % (param['N_samples_Q'], param['N_samples_P'], \n",
    "                                                            param['random_seed'], param['exp_no'])\n",
    "filename = main_dir + '/assets/' + param['dataset']+'/%s.pickle' % (param['expname'])\n",
    "\n",
    "if param['plot_intermediate_result'] == True:\n",
    "    if 'gaussian' in param['dataset'] and 'Extension' not in param['dataset']:\n",
    "         r_param = param['sigma_Q']\n",
    "    elif 'student_t' in param['dataset']:\n",
    "        r_param = param['nu']\n",
    "    elif param['dataset'] == 'Extension_of_gaussian':\n",
    "        r_param = param['a']\n",
    "    else:\n",
    "        r_param = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa421b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter     50: loss = -0.0000367761, norm of dW = 0.00, kinetic energy of P = 0.0000027737, average learning rate for P = 1.000000\n",
      "grad 0.0021760582\n",
      "iter    100: loss = 0.0000112057, norm of dW = 0.00, kinetic energy of P = 0.0000138084, average learning rate for P = 1.000000\n",
      "grad 0.0052533494\n",
      "iter    150: loss = 0.0005307198, norm of dW = 0.00, kinetic energy of P = 0.0000257846, average learning rate for P = 1.000000\n",
      "grad 0.00671273\n",
      "iter    200: loss = -0.0001094341, norm of dW = 0.01, kinetic energy of P = 0.0001318022, average learning rate for P = 1.000000\n",
      "grad 0.010807219\n",
      "iter    250: loss = -0.0001478195, norm of dW = 0.00, kinetic energy of P = 0.0000672681, average learning rate for P = 1.000000\n",
      "grad 0.011518562\n",
      "iter    300: loss = 0.0000367165, norm of dW = 0.00, kinetic energy of P = 0.0000173353, average learning rate for P = 1.000000\n",
      "grad 0.0054884893\n",
      "iter    350: loss = 0.0000181198, norm of dW = 0.00, kinetic energy of P = 0.0000034126, average learning rate for P = 1.000000\n",
      "grad 0.0016046577\n",
      "iter    400: loss = -0.0000486374, norm of dW = 0.01, kinetic energy of P = 0.0000188504, average learning rate for P = 1.000000\n",
      "grad 0.006118468\n",
      "iter    450: loss = -0.0001130104, norm of dW = 0.00, kinetic energy of P = 0.0000183497, average learning rate for P = 1.000000\n",
      "grad 0.006058007\n",
      "iter    500: loss = 0.0001592636, norm of dW = 0.00, kinetic energy of P = 0.0001395879, average learning rate for P = 1.000000\n",
      "grad 0.016557546\n",
      "total time 32.838s\n"
     ]
    }
   ],
   "source": [
    "# Train ---------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "t0 = time.time()\n",
    "\n",
    "for it in range(1, param['epochs']+1): # Loop for updating particles P\n",
    "    parameters, current_loss, dW_norm = train_disc(parameters, phi, P, Q, lr_phi, param['epochs_phi'],\n",
    "                                                   loss_par, NN_par, data_par, param['optimizer'], \n",
    "                                                   print_vals=True)\n",
    "    \n",
    "    if param['calc_Wasserstein1'] == True:\n",
    "        parameters2, current_wass1, _ = train_wasserstein1(parameters2, phi2, P, Q, lr_phi, \n",
    "                                                           param['epochs_phi'], NN_par2, data_par, \n",
    "                                                           param['optimizer'], print_vals=True)\n",
    "    \n",
    "    dPs.append( calc_vectorfield(phi, P, parameters, NN_par, loss_par, data_par) )\n",
    "    P, dPs, dP = solve_ode(P, lr_P, dPs, param['ode_solver'], aux_params) # update P\n",
    "\n",
    "    lr_Ps.append(lr_P.numpy())\n",
    "    \n",
    "    # save results\n",
    "    divergences.append(current_loss)\n",
    "    KE_P = calc_ke(dP, param['N_samples_P'])\n",
    "    KE_Ps.append(KE_P)\n",
    "    grad_phi = calc_grad_phi(dP)\n",
    "    #print(\"grad\", grad_phi)\n",
    "    if param['calc_Wasserstein1'] == True:\n",
    "        wasserstein1s.append(current_wass1)\n",
    "    \n",
    "    if param['epochs']<=100 or it%param['save_iter'] == 0:\n",
    "        trajectories.append(P.numpy())\n",
    "        if np.prod(param['N_dim']) < 500:\n",
    "            vectorfields.append(dP.numpy())\n",
    "        elif np.prod(param['N_dim']) >= 784:  # image data\n",
    "            FIDs.append( calc_fid(pred=P.numpy(), real=Q.numpy()) )\n",
    "    \n",
    "    \n",
    "    if it % (param['epochs']/10) == 0:\n",
    "        display_msg = 'iter %6d: loss = %.10f, norm of dW = %.2f, ' % (it, current_loss, dW_norm)\n",
    "        display_msg = display_msg + 'kinetic energy of P = %.10f, ' % KE_P\n",
    "        display_msg = display_msg + 'average learning rate for P = %.6f' % tf.math.reduce_mean(lr_P).numpy()\n",
    "        if len(FIDs) > 0 :\n",
    "            display_msg = display_msg + ', FID = %.3f' % FIDs[-1]   \n",
    "        print(display_msg)\n",
    "        print(\"grad\", grad_phi)\n",
    "        \n",
    "        if param['plot_intermediate_result'] == True:\n",
    "            data = {'trajectories': trajectories, 'divergences': divergences, 'wasserstein1s':wasserstein1s, \n",
    "                    'KE_Ps': KE_Ps, 'FIDs':FIDs, 'X_':X_, 'Y_':Y_, 'X_label':X_label, 'Y_label':Y_label,\n",
    "                    'dt': lr_Ps, 'dataset': param['dataset'], 'r_param': r_param, 'vectorfields': vectorfields, \n",
    "                    'save_iter':param['save_iter']}\n",
    "            if param['N_dim'] ==2:\n",
    "                data.update({'phi': phi, 'W':W, 'b':b, 'NN_par':NN_par})\n",
    "            plot_result(filename, intermediate=True, epochs = it, iter_nos = None, data = data, show=False)\n",
    "            \n",
    "\n",
    "total_time = time.time() - t0\n",
    "print(f'total time {total_time:.3f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a4e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved at: /Users/hyemin/Documents/source_code/Lipschitz-regularized-GPA-github/assets/Learning_gaussian/KL-Lipschitz_1.0000_1.00_0200_0200_00_a_trial.pickle\n"
     ]
    }
   ],
   "source": [
    "# Save result ------------------------------------------------------\n",
    "import pickle\n",
    "if param['N_dim'] == 1:\n",
    "    X_ = np.concatenate((X_, np.zeros(shape=X_.shape)), axis=1)\n",
    "    Y_ = np.concatenate((Y_, np.zeros(shape=Y_.shape)), axis=1)\n",
    "    \n",
    "    trajectories = [np.concatenate((x, np.zeros(shape=x.shape)), axis=1) for x in trajectories]\n",
    "    vectorfields = [np.concatenate((x, np.zeros(shape=x.shape)), axis=1) for x in vectorfields]\n",
    "  \n",
    "if param['L'] == None:\n",
    "    param['L'] = 'inf'\n",
    "param.update({'X_': X_, 'Y_': Y_, 'lr_Ps':lr_Ps,})\n",
    "result = {'trajectories': trajectories, 'vectorfields': vectorfields, 'divergences': divergences, 'KE_Ps': KE_Ps, 'FIDs': FIDs, 'wasserstein1s': wasserstein1s}\n",
    "        \n",
    "# Save trained data\n",
    "with open(filename,\"wb\") as fw:\n",
    "    pickle.dump([param, result] , fw)\n",
    "print(\"Results saved at:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d847fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final result ------------------------------------------------------\n",
    "from scripts.util.plot_result import plot_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
