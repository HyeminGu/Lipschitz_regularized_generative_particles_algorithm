# configuration for 3D_Swiss_roll example
# whose discriminator is parametrized by NN
# null converts to None in Python
---
# Data
N_samples_Q: 200        # total number of target samples
N_samples_P: 200        # total number of prior samples
N_dim: 3                # dimension of input data
dataset: 3D_Swiss_roll
random_seed: 0          # random seed for data generator

N_latent_dim: null      # dimension of latent space
sample_latent: False    # True: sample in the latent space, False: sample in the physical space

N_project_dim: null     # dimension of PCA projected space on input

beta: null              # gibbs distribution of -|x|^\beta
sigma_P: 3.0            # std of initial gaussian distribution
sigma_Q: null            # std of target gaussian distribution
nu: null                # df of target student-t distribution
interval_length: null   # interval length of the uniform distribution
label: null             # class label of image data
pts_P: null             # x-coordinate value of points ex) [10.0,]
pts_Q: null             # x-coordinate value of points ex) [0.0,]
pts_P_2: null           # y-coordinate value of points ex) [0.0,]
pts_Q_2: null           # y-coordinate value of points ex) [0.0,]
y0: null                # Lorenz equation initial value ex) [1.0,2.0, 2.0]

# (f, Gamma)-divergence
f: KL                   # choices=['KL', 'alpha', 'reverse_KL', 'JS']
alpha: null             # parameter value for alpha divergence
formulation: LT         # LT or DV in case of f=KL, otherwise, keep LT
Gamma: Lipschitz        # function space choices=['Lipshitz']
L: 1.0                  # Lipshitz constant: default=inf w/o constraint
reverse: False          # True -> D(Q|P), False -> D(P|Q)
constraint: hard        # choices=['hard', 'soft']
lamda: null             # coefficient on soft constraint
generative_model: GPA_NN# generative model name

# Wasserstein-1 metric
calc_Wasserstein1: False # True -> check Wasserstein-1 metric

# Neural Network discriminator <phi>
NN_model: fnn           # choices=['fnn', 'cnn', 'cnn-fnn']
N_fnn_layers:           # list of the number of FNN hidden layer units / the number of CNN feed-forward hidden layer units
    - 32
    - 32
    - 32
N_cnn_layers: null      # list of the number of CNN channels
activation_ftn:
    - relu    # [0]: for the fnn/convolutional layer, [1]: for the cnn feed-forward layer, [2]: for the LAST cnn feed-forward layer'
# choices=['relu', 'mollified_relu_cos3','mollified_relu_poly3','mollified_relu_cos3_shift','softplus', 'leaky_relu','elu', 'bounded_relu', 'bounded_elu']
eps: 0.5                # Mollifier shape adjusting parameter when using mollified relu3 activations
N_conditions: 1         # number of classes for the conditional setting

# Discriminator training parameters
epochs_phi: 3           # numbef of updates for NN to find phi*
lr_phi: 0.001            # lr for NN
optimizer: adam         # optimizer for NN, choices=['sgd', 'adam',]

# Particles transportation parameters
epochs: 5000            # number of updates for P
lr_P: 0.5               # lr for P
ode_solver: Heun   # ode solver for particle ode, choices=['forward_euler', 'AB2', 'AB3', 'AB4', 'AB5', 'ABM1', 'Heun', 'ABM2', 'ABM3', 'ABM4', 'ABM5', 'RK4', 'ode45' ]
mobility: null          # problem dependent mobility function\nRecommendation: MNIST - bounded
lr_P_decay: null        # delta t decay, choices=['rational', 'step',]

exp_no: a_trial         # short experiment name under the same data
mb_size_P: 200          # mini batch size for the moving distribution P
mb_size_Q: 200          # mini batch size for the target distribution Q

# save/display
save_iter: 50            # save results per each save_iter
plot_result: True       # True -> show plots
plot_intermediate_result: False # True -> save intermediate plots
...
